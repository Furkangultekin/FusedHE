<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>FusedHE â€” Monocular Height Estimation</title>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
<style>
:root{
  --bg:#ffffff;
  --text:#111;
  --muted:#5f6368;
  --border:#e7e8ea;
  --card:#fafafa;
  --radius:16px;
  --maxw:1100px;
}
*{box-sizing:border-box;scroll-behavior:smooth;}
body{margin:0;padding:0;background:var(--bg);color:var(--text);
  font-family:Inter,system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif;line-height:1.6}
.container{max-width:var(--maxw);margin:0 auto;padding:24px}
header{text-align:center;padding:48px 0 24px}
h1{margin:0 0 8px;font-size:clamp(28px,4vw,40px);letter-spacing:.2px}
.subtitle{color:var(--muted);max-width:900px;margin:0 auto}
p {
  text-align: justify;
  text-justify: inter-word;
}
.badge{font-size:13px;font-weight:600;background:#000;color:#fff;
  padding:3px 8px;border-radius:999px;margin-left:6px}
.button-bar{margin-top:24px;display:flex;flex-wrap:wrap;gap:12px;justify-content:center}
.btn{
  background:#000;color:#fff;padding:12px 24px;border-radius:25px;
  text-decoration:none;font-weight:600;display:inline-flex;align-items:center;gap:8px;
  border:2px solid #000;transition:.25s all ease
}
.btn:hover{background:#fff;color:#000}

/* Navigation Tabs */
.navbar{
  display:flex;flex-wrap:wrap;justify-content:center;gap:12px;
  margin-top:28px;margin-bottom:20px;
}
.navbar a{
  padding:8px 16px;border:1px solid var(--border);
  border-radius:25px;text-decoration:none;color:var(--muted);
  font-weight:500;transition:.2s;
}
.navbar a:hover{background:#000;color:#fff;border-color:#000}

/* Layout sections */
.section{background:var(--card);border:1px solid var(--border);
  border-radius:var(--radius);padding:22px;margin:12px 0}
.full{grid-column:1 / -1}
.grid-2{display:grid;grid-template-columns:1fr 1fr;gap:16px}
@media (max-width:900px){.grid-2{grid-template-columns:1fr}}
.resp-img { width: 90%; height: auto; display: block; }
@media (min-width: 900px) {
  .desk-50 { width: 50%; }
  .desk-60 { width: 60%; }
  .desk-70 { width: 70%; }
  .desk-80 { width: 80%; }
  .desk-90 { width: 90%; }
  .center  { margin-left: auto; margin-right: auto; }
}
.section h2{margin:0 0 8px;font-size:22px}
.media img{max-width:100%;height:auto;display:block;border-radius:12px;border:1px solid var(--border)}
.caption{text-align:center;color:var(--muted);font-size:13px;margin-top:6px}
table{width:100%;border-collapse:collapse;border:1px solid var(--border);border-radius:12px;overflow:hidden}
thead th{background:#f6f7f8;text-align:left;border-bottom:1px solid var(--border);padding:10px}
tbody td{border-bottom:1px solid var(--border);padding:10px}
tbody tr:last-child td{border-bottom:none}
footer{text-align:center;color:var(--muted);padding:32px 0}
/* Modal (pop-up) */
.modal-backdrop{
  position:fixed; inset:0; background:rgba(0,0,0,.45);
  display:none; align-items:center; justify-content:center; z-index:9999;
}
.modal{
  background:#fff; color:#111; width:min(520px,92vw);
  border:1px solid #e7e8ea; border-radius:16px;
  box-shadow:0 12px 32px rgba(0,0,0,.2); overflow:hidden;
  font-family:Inter,system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif;
}
.modal header{
  display:flex; align-items:center; justify-content:space-between;
  padding:14px 18px; border-bottom:1px solid #e7e8ea; font-weight:700;
}
.modal .content{ padding:22px 20px; text-align:center; }
.modal .content p{ margin:0; font-weight:600; line-height:1.5; }
.modal .actions{ padding:14px 18px; border-top:1px solid #e7e8ea; text-align:right; }
.modal .x{
  background:transparent; border:0; font-size:20px; line-height:1; cursor:pointer; padding:2px 6px;
}
</style>
</head>
<body>
<div class="container">
<header>
  <h1>FusedHE <span class="badge">Monocular Height Estimation</span></h1>
  <p class="subtitle"><strong>Fused-HE: Fusing Convolution and Vision Transformers Encoder for Object Height Estimation from Monocular Satellite and Aerial Images</strong></p>

  <div class="button-bar">
    <a class="btn" href="#" data-modal="Paper">ðŸ“„ Paper</a>
    <a class="btn" href="#" data-modal="Supplementary">ðŸ“š Supplementary</a>
    <a class="btn" href="https://github.com/Furkangultekin/FusedHE" target="_blank">ðŸ’» Code</a>
    <a class="btn" href="#" data-modal="Poster">ðŸ–¼ Poster</a>
  </div>

  <!-- Navigation Tabs -->
  <nav class="navbar">
    <a href="#overview">Overview</a>
    <a href="#features">Features</a>
    <a href="#models">Models</a>
    <a href="#datasets">Datasets</a>
    <a href="#metrics">Metrics</a>
    <a href="#visual">Visuals</a>
    <a href="#profile">Profile</a>
    <a href="#generalization">Generalization</a>
  </nav>
</header>

<!-- Overview -->
<section id="overview" class="section full">
  <h2>Overview</h2>
  <p>
    Fused-HE introduces a hybrid deep learning architecture designed to infer fine-grained height information 
    from single-view (monocular) satellite and aerial imagery. The model unifies Convolutional Neural Networks 
    (CNNs) which excel at capturing local spatial textures, with Vision Transformer (ViT) encoders that model 
    long-range global dependencies. By fusing these complementary representations within a shared encoder space, 
    Fused-HE enables robust height estimation without requiring stereo or multi-view data. Furthermore, 
    the Fused-HE model has been extended with a segmentation head (FusedSeg-HE) to enhance boundary precision and 
    overall height estimation accuracy. This approach provides a scalable and data-efficient solution for large-area 
    3D reconstruction, urban analysis, and geospatial mapping tasks, outperforming standalone convolutional or 
    transformer-based baselines.
  </p>
  <div class="media" style="margin-bottom:12px;">
    <img class="resp-img desk-80 center" src="assets/3d_barca.jpg"  alt="3D visual result" style="margin:auto;display:block;">

    <img class="resp-img desk-80 center" src="assets/3d_2.jpg"  alt="3D visual result" style="margin:auto;display:block;">
  </div>
</section>

<!-- Features + Models -->
<div class="grid-2">
  <section id="features" class="section">
    <h2>Features</h2>
    <ul>
      <li><strong>Hybrid Architecture:</strong> Combines CNN local extraction with ViT global context.</li>
      <li><strong>Monoscopic Input:</strong> Requires only one image, no stereo pairs.</li>
      <li><strong>Height Estimation:</strong> Generates per-pixel height predictions for geospatial tasks.</li>
    </ul>
  </section>

  <section id="models" class="section">
    <h2>Models</h2>
    <ul>
      <li><strong>CNN:</strong> Height estimation using ResNet-101 encoder (<a href="https://arxiv.org/abs/1512.03385" target="_blank">paper</a>).</li>
      <li><strong>MiT:</strong> SegFormerâ€™s hierarchical ViT (MiT) encoder (<a href="https://arxiv.org/abs/2105.15203" target="_blank">paper</a>).</li>
      <li><strong>Fused-HE:</strong> Fusion of convolutional and transformer encoders (<a href="https://open.metu.edu.tr/handle/11511/108758" target="_blank">link</a>).</li>
      <li><strong>FusedSeg-HE:</strong> Fused encoder with segmentation head for higher accuracy (<a href="https://open.metu.edu.tr/handle/11511/108758" target="_blank">link</a>).</li>
    </ul>
  </section>
</div>

<!-- Datasets -->
<section id="datasets" class="section full">
  <h2>Datasets</h2>
  <ul>
    <li><strong>Data Fusion Contest 2019 (DFC2019):</strong> WorldView-3 imagery (Jacksonville, Omaha) with LiDAR-derived nDSM ground truth.</li>
    <li><strong>Data Fusion Contest 2023 (DFC2023):</strong> SuperView-1, Gaofen-2/3 imagery from 17 cities across 6 continents.</li>
    <li><strong>Ground Truth:</strong> nDSM (normalized Digital Surface Model) used for pixel-wise height supervision.</li>
  </ul>
  <div class="media" style="margin-top:10px;">
    <img class="resp-img desk-70 center" src="assets/ndsm.jpg" alt="nDSM ground truth example" style="margin:auto;display:block;">
  </div>
  <p class="caption">Example nDSM visualization used as supervision target. DTM: Digital Terrain Model, 
    DSM: Digital Surface Model, nDSM: Normalized Digital Surface Model</p>
</section>

<!-- Metric Results -->
<section id="metrics" class="section full">
  <h2>Metric Results</h2>
  <p>Results of different configurations of Fused-HE, FusedSeg-HE, and baseline methods on the DFC2023 dataset. 
    Arrows indicate whether higher or lower values are preferable, with the best results in bold and the second-best 
    underlined.  Results of models trained from scratch are presented in the upper block, while results of models 
    fine-tuned with pre-trained weights are shown in the lower block.</p>
  <table>
    <thead>
      <tr>
        <th>Method</th><th>Pre-trained</th><th>RMSE (m) â†“</th><th>IoU â†‘</th><th>&delta;&lt;1.25 â†‘</th><th>&delta;&lt;1.25<sup>2</sup> â†‘</th>
        <th>&delta;&lt;1.25<sup>3</sup> â†‘</th>
      </tr>
    </thead>
    <tbody>
      <tr><td>CNN-HE</td><td>âœ—</td><td>4.286</td><td>0.583</td><td>0.706</td><td>0.755</td><td>0.782</td></tr>
      <tr><td>ViT-HE</td><td>âœ—</td><td>4.812</td><td>0.539</td><td>0.696</td><td>0.742</td><td>0.769</td></tr>
      <tr><td>Depth Anything</td><td>âœ—</td><td>4.611</td><td>0.553</td><td>0.671</td><td>0.718</td><td>0.748</td></tr>
      <tr><td>MiT-HE</td><td>âœ—</td><td><u>4.253</u></td><td><u>0.594</u></td><td><u>0.714</u></td><td><u>0.760</u></td><td><u>0.785</u></td></tr>
      <tr><td>Fused-HE-RV</td><td>âœ—</td><td>4.792</td><td>0.546</td><td>0.709</td><td>0.758</td><td>0.785</td></tr>
      <tr style="border-bottom:3px solid #000;"><td ><b>FusedSeg-HE-RM</b></td><td>âœ—</td><td><b>4.206</b></td><td><b>0.598</b></td><td><b>0.719</b></td><td><b>0.765</b></td><td><b>0.789</b></td></tr>
      <tr><td>CNN-HE</td><td>âœ“</td><td>3.854</td><td>0.645</td><td>0.767</td><td>0.811</td><td>0.833</td></tr>
      <tr><td>ViT-HE</td><td>âœ“</td><td>3.801</td><td>0.670</td><td>0.787</td><td>0.832</td><td>0.854</td></tr>
      <tr><td>MiT-HE</td><td>âœ“</td><td>3.490</td><td>0.689</td><td>0.792</td><td>0.835</td><td>0.857</td></tr>
      <tr><td>Depth Anything</td><td>âœ“</td><td>3.407</td><td><u>0.715</u></td><td>0.802</td><td>0.846</td><td>0.867</td></tr>
      <tr><td>Fused-HE-RV</td><td>âœ“</td><td>3.730</td><td>0.659</td><td>0.774</td><td>0.826</td><td>0.855</td></tr>
      <tr><td>Fused-HE-RM</td><td>âœ“</td><td><u>3.379</u></td><td>0.706</td><td><u>0.810</u></td><td><u>0.850</u></td><td><u>0.871</u></td></tr>
      <tr><td>FusedSeg-HE-RM</td><td>âœ“</td><td><b>3.346</b></td><td><b>0.739</b></td><td><b>0.836</b></td><td><b>0.878</b></td><td><b>0.897</b></td></tr>

    </tbody>
  </table>
  <p class="caption">Metric results comparison of the baseline models and proposed models trained with DFC2023 Dataset</p>
</section>

<!-- Visual Results -->
<section id="visual" class="section full">
  <h2>Visual Results</h2>
  <p>The examples below show that CNN-based models capture sharp structures, while ViT-based ones provide 
    smoother and more consistent predictions. Fused-HE combines both advantages, and FusedSeg-HE further 
    refines height estimation near object boundaries.</p>
  <div class="media" style="margin-bottom:12px;">
    <img class="resp-img desk-90 center" src="assets/visual_23.jpg" alt="Qualitative visual results overview" style="margin:auto;display:block;">

    <img class="resp-img desk-90 center" src="assets/vis_23_detail.jpg" alt="Visual results detailed crop" style="margin:auto;display:block;">
  </div>
<!-- Building Height Profile -->
<section id="profile" class="section full">
  <h2>Building Height Profile</h2>
  <p>The figure below shows FusedSeg-HE height profiles (trained on DFC2023) for three regions, compared with ground truth. 
    The model reliably identifies building locations and assigns heights to the correct pixels, but its outputs are smoother, 
    underrepresenting sharp, short-range height changes. Additional 3D visualizations appear in the supplementary material.</p>
  <div class="media">
    <img class="resp-img desk-80 center" src="assets/height_profile_new.jpg" alt="Building height profile" style="margin:auto;display:block;">
  </div>
  <p class="caption">Estimated per-building height profiles along selected transects.</p>
</section>

<!-- Generalization -->
<section id="generalization" class="section full">
  <h2>Generalizability</h2>
  <p>We aim to develop height estimation methods for satellite and aerial imagery that 
    transfer robustly across datasets and sensors. The figure below shows outputs of FusedSeg-HE, trained on 
    nadir-view scenes from DFC2023, applied to diverse sensors and real-world conditions. On pre/post-disaster WorldView-3 
    pairs, the model clearly recovers building-height changes; in post-flood imagery, height contrasts between affected 
    and unaffected areas are distinct. Performance on high-resolution aerial images, including off-nadir views, remains 
    consistent and yields plausible height maps. Overall, these results indicate strong robustness and practical 
    applicability for disaster management, urban planning, and environmental monitoring.
</p>
  <div class="media">
    <img class="resp-img desk-50 center" src="assets/inference.jpg" alt="Inference and generalization examples" style="margin:auto;display:block;">
  </div>
  <p class="caption">Results of the FusedSeg-HE-RM model on images from
the WV3 satellite before and after the earthquake in Turkey,
with 30 cm GSD (a, b), WV3 after flooding in Libya (c), nadir
and oblique aerial images with 13 cm and 10 cm GSD from the
SkyScapes and EAGLE datasets (d, e), and an ortho aerial
image with 5 cm GSD from the Potsdam dataset (f). The scale
bars represent height values in meters.</p>
</section>

<div id="releaseModal" class="modal-backdrop" aria-hidden="true">
  <div class="modal" role="dialog" aria-modal="true" aria-labelledby="modalTitle">
    <header>
      <span id="modalTitle">Coming Soon</span>
      <button class="x" aria-label="Close" id="modalX">Ã—</button>
    </header>
    <div class="content">
      <p>
      THE PAPER, SUPPLEMENTARY, AND POSTER <br>
      WILL BE RELEASED AFTER THE ICCV 2025 CONFERENCE.
      </p>
    </div>
    <div class="actions">
      <button class="btn" id="modalClose">OK</button>
    </div>
  </div>
</div>


<footer>
  Â© <span id="y"></span> FusedHE â€” Monocular Height Estimation
</footer>
</div>
<script>document.getElementById('y').textContent=new Date().getFullYear();</script>
<script>
  const backdrop = document.getElementById('releaseModal');
  const titleEl  = document.getElementById('modalTitle');
  const closeBtn = document.getElementById('modalClose');
  const xBtn     = document.getElementById('modalX');

  // Paper / Supplementary / Poster butonlarÄ±na tÄ±klandÄ±ÄŸÄ±nda modalÄ± aÃ§
  document.querySelectorAll('[data-modal]').forEach(el => {
    el.addEventListener('click', e => {
      e.preventDefault();
      titleEl.textContent = el.dataset.modal;
      backdrop.style.display = 'flex';
      backdrop.setAttribute('aria-hidden','false');
    });
  });

  function closeModal(){
    backdrop.style.display = 'none';
    backdrop.setAttribute('aria-hidden','true');
  }

  // Kapatma davranÄ±ÅŸlarÄ±
  closeBtn.addEventListener('click', closeModal);
  xBtn.addEventListener('click', closeModal);
  backdrop.addEventListener('click', (e) => {
    if (e.target === backdrop) closeModal(); // arka plana tÄ±klayÄ±nca kapanÄ±r
  });
  document.addEventListener('keydown', (e) => {
    if (e.key === 'Escape') closeModal();    // ESC ile kapanÄ±r
  });
</script>

</body>
</html>
